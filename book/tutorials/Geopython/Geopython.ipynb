{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with multidimensional spatial data in Python\n",
    "\n",
    "## Overview \n",
    "\n",
    "In this notebook, we will use be introduced to the foundational packages for working with spatial data in python, and learn how to use these to manipulate spatial data.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Use `numpy` to manipulate array data\n",
    "2. Use `pandas` to manipulate tabular data\n",
    "3. Apply basic geospatial manipulations to vector data using `geopandas`\n",
    "3. Work with multidimensional gridded data in `xarray`\n",
    "3. Perform geospatial operations on `xarray`s using `rioxarray`\n",
    "\n",
    "## Requirements\n",
    "This tutorial requires the following Python modules installed `numpy`, `pandas`,`geopandas`, `xarray`,`rioxarray`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foundational Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6X4WJo3iM6m9"
   },
   "source": [
    "#### NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_H3bNbLloXCY"
   },
   "source": [
    "NumPy is a popular library for storing arrays of numbers and performing computations on them. Not only does it enable writing more succinct code, it also makes the code faster, since most NumPy routines are implemented in C for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcq006V9Z7aX"
   },
   "source": [
    "Numpy is probably the most important package for doing data science and scientific analysis in Python. It is very powerful and is often the basis for other packages that add additional complexity (it is at the core of all raster handling packages in python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7tI3XLhqwSX"
   },
   "source": [
    "To use NumPy in your program, you need to import it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "phSPPyfyq2gX"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9secCfFLNHEE"
   },
   "source": [
    "##### Array creation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSS2wEnkq97n"
   },
   "source": [
    "NumPy arrays can be created from Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfeg286yrLvJ"
   },
   "outputs": [],
   "source": [
    "my_array = np.array([1, 2, 3])\n",
    "my_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy2EvrxFriAG"
   },
   "source": [
    "NumPy supports arrays of arbitrary dimension. For example, we can create two-dimensional arrays (e.g. to store a matrix) as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wM-GYVMsrzNs"
   },
   "outputs": [],
   "source": [
    "my_2d_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "my_2d_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kZMzYsAsVAc"
   },
   "source": [
    "We can access individual elements of a 2d-array using two indices as indexing  works just like for Python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4q8X86BbscPd"
   },
   "outputs": [],
   "source": [
    "# index using:\n",
    "# position_in_dim1, position_in_dim2\n",
    "my_2d_array[1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfVIKyxkTh0p"
   },
   "source": [
    "We can also access rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrKnDAtyTlYe"
   },
   "outputs": [],
   "source": [
    "my_2d_array[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hskLBCp9ToCG"
   },
   "source": [
    "and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOOFsLHhTozX"
   },
   "outputs": [],
   "source": [
    "my_2d_array[:, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34_P7-1TraYJ"
   },
   "source": [
    "a bare index of `:` means give me everything in that dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqlKtZ8ErgMY"
   },
   "outputs": [],
   "source": [
    "print(my_2d_array[:, :])\n",
    "#is the same as\n",
    "print(my_2d_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keWK_5PHr9Q2"
   },
   "source": [
    "Arrays have a `shape` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QIo7l1Yr8m7"
   },
   "outputs": [],
   "source": [
    "print(my_array.shape)\n",
    "print(my_2d_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9B0iCBlmfeY"
   },
   "source": [
    "##### Basic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elQGgkqDxKLV"
   },
   "source": [
    "In NumPy, we express computations directly over arrays. This makes the code much more succint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkCU1T8ixghX"
   },
   "source": [
    "Arithmetic operations can be performed directly over arrays. For instance, assuming two arrays have a compatible shape, we can add them as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AoiRq42x5mI"
   },
   "outputs": [],
   "source": [
    "array_a = np.array([1, 2, 3])\n",
    "array_b = np.array([4, 5, 6])\n",
    "array_a + array_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qdn8MwpR0wX_"
   },
   "source": [
    "NumPy has a range of functions that can be applied to arrays, e.g. `np.sin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoanjiMu1BH5"
   },
   "outputs": [],
   "source": [
    "np.sin(array_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odVawD9m6gwv"
   },
   "source": [
    "numpy arrays also have their own methods e.g matrix transpose `.transpose()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvPe_JAO6mvF"
   },
   "outputs": [],
   "source": [
    "array_a.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPMuyOupZlUL"
   },
   "source": [
    "Because numpy arrays can have many dimensions, indexing can get quite complicated. If you want to know more have a look at the docs [here](https://numpy.org/doc/stable/user/basics.indexing.html). Lets look at what this looks like on a 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e24LPeHWbo-p"
   },
   "outputs": [],
   "source": [
    "# Create a 3D array with values ranging from 0 to 269 with shape 3x3x30.\n",
    "array_3d = np.arange(270).reshape(3, 3, 30)\n",
    "array_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlYYAIMMcSCw"
   },
   "source": [
    "Lets pretend that this array has dimension latitude, longitude and band (3x3x30). Now lets extract a spatial subset  - we only want the first 2x2 lat long grid with all wavelengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc12K9dtcCb6"
   },
   "outputs": [],
   "source": [
    "#dimensions separated by comma\n",
    "#1st dim - :2 indicates we want everything up to the 2nd coordinate i.e 0 and 1. Writing 0:2 would be equivalent\n",
    "#2nd dim - sanme as above\n",
    "#3rd dim -  : a bare semicolon indicate we want all entires in that dimension\n",
    "sliced_array = array_3d[:2, :2, :]\n",
    "sliced_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-KSTGPei5J_"
   },
   "source": [
    "#### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnatreATjmln"
   },
   "source": [
    "\n",
    "Pandas is the Python library used for tabular data manipulation and analysis. It is **very** widely used. It is the Python version of dplyr, tidyr and readr rolled into one.\n",
    "\n",
    "Here's an example of loading a DataFrame from a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RV3RfR8Yj-XP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data from a CSV file into a DataFrame\n",
    "df = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51ESmFsoi8NK"
   },
   "source": [
    "There are lots and lots of tutorial on how to wrnagle data with pandas. I will just show you some basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQgNz9Cbkllu"
   },
   "source": [
    "##### Indexing\n",
    "Or how to select particular rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPKQbPJXkqv8"
   },
   "outputs": [],
   "source": [
    "# Accessing a column\n",
    "print(df['sepal_length'])\n",
    "\n",
    "# use .loc to access a row by label\n",
    "print(df.loc[0])\n",
    "\n",
    "# use .iloc to access a row by integer index\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB35UdoolC9Y"
   },
   "source": [
    "##### Filtering\n",
    "or selecting rows by a logical condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "neNkAFkBlJzc"
   },
   "outputs": [],
   "source": [
    "# Filter rows where 'sepal_length' is greater than 5\n",
    "filtered = df[df['sepal_length'] > 5]\n",
    "\n",
    "#what is happening here is that df['sepal_length'] > 5 is creating a series of True/False values based on the condition\n",
    "#this is then used to retrun rows where the series is True\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQMlU2Rwm0BG"
   },
   "source": [
    "another way of achieving the same thing is using `query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyx2_Bzfm453"
   },
   "outputs": [],
   "source": [
    "filtered = df.query('sepal_length > 5')\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp1Cw-Dil7Fo"
   },
   "source": [
    "##### Grouping and aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3V5leo-l-k4"
   },
   "outputs": [],
   "source": [
    "# Group data by the values in 'species' and compute the mean of the other columns for each group\n",
    "grouped = df.groupby('species').mean()\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBe3buVKmGB0"
   },
   "outputs": [],
   "source": [
    "# or maybe we just want the aggregation of a single colmn\n",
    "petal_grouped = df.groupby('species')['petal_length'].mean()\n",
    "print(petal_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOWHhHoAphdd"
   },
   "source": [
    "is the same as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UC2evyQspjlj"
   },
   "outputs": [],
   "source": [
    "#we can use the .mean() method without explicitly assinging the result of the preceding function\n",
    "#the trick is that you have to know the the type/class of the first function is to know what methods it has\n",
    "#but with pandas the results are usually another dataframe\n",
    "result = df.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVCl_IhYpog7"
   },
   "source": [
    "Lets see how we can chain many things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gZiWSP7ml1E"
   },
   "outputs": [],
   "source": [
    "#lets do everything above in one step\n",
    "result = (pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "          .query('sepal_length > 5')\n",
    "          .groupby('species')\n",
    "          .mean())\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial data handling\n",
    "A number of packages exist for working with geospatial data in Python. Some are more widely used than others. GeoPandas is the standard for working with spatial vector data. For working with raster data traditionally Rasterio has been the standard. Rasterio is a wrapper for GDAL which you may already be familiar with. Rasterio is not well suited to working with data with more than 2 dimensions (lat/long) or with a large number of bands. Xarray is well suited to high-dimensional data, and is rapidly growing in popularity. Hence this is what we will focus on. We will also spend a fair bit of time learning about dask, a package for making xarray (and Python in general) scale to large datasets.\n",
    "\n",
    "#### Vector data with GeoPandas\n",
    "[GeoPandas](https://geopandas.org/en/stable/getting_started.html) extends the datatypes used by pandas to allow spatial operations on geometric types. If you have worked with sf in R, you will find Geopandas very familiar. Underneath, Geopandas uses GEOS for geometric calculcation via the shapely package. In R, sf also uses GEOS.\n",
    "\n",
    "The core data structure in GeoPandas is the `geopandas.GeoDataFrame`, a subclass of `pandas.DataFrame` that can store geometry columns and perform spatial operations.\n",
    "A GeoDataFrame is a combination of regular pandas columns (`pandas.Series`), with traditional data and a special geometry column (`geopandas.GeoSeries`), with geometries (points, polygons, etc.). The geometry column has a `GeoSeries.crs` attribute, which stores information about the projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](geopandas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will explore 2 datasets, `swfynbos.gpkg`, a dataset of the vegetation types of the southwestern Cape of South Africa, and `fynbos_remnants`, a dataset of the remaining fragments of natural vegetation in this region. This data is in the geopackage format but Geopandas can open all commonly encountered geospatial vector data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we typically use the alias gpd\n",
    "import geopandas as gpd\n",
    "\n",
    "#read file\n",
    "vegtypes = gpd.read_file('shared/users/gmoncrieff/data/swfynbos.gpkg')\n",
    "\n",
    "#view some rows\n",
    "vegtypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into some data manipulations, lets looks at some attributes of the data. Geopandas allows us to easlity access some relevant attributes of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the type of each geometry\n",
    "print(vegtypes.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#area of each polygon\n",
    "print(vegtypes.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroid of each polygon\n",
    "print(vegtypes.centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the coordinate reference system of the geodataframe using `.crs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegtypes.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing things like filters, selecting columns and rows, etc works exactly like a Pandas dataframe, as a geodataframe is a subclass of a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select first 5 rows\n",
    "print(vegtypes.iloc[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to a single vegtypes\n",
    "print(vegtypes.query('Name_18 == \"Hangklip Sand Fynbos\"').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plotting is easy too. Like Pandas there is a handy `.plot()` method for geodataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colour plot by vegetation type\n",
    "vegtypes.plot('Name_18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start playing with maniplating geodataframes based on their geometries, let's load another dataset that we will combine with the first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remnants = gpd.read_file('data/remnants.gpkg')\n",
    "#lets view the first few rows\n",
    "remnants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set operations like intersections and unions can be applied using the `gpd.overlay()` function. Let's extract the remaining natural vegetation of each vegetation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersection of vegtypes and remnants\n",
    "veg_remnants = gpd.overlay(vegtypes,remnants,how='intersection')\n",
    "\n",
    "#plot!\n",
    "veg_remnants.plot('Name_18')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing set operations, the properties from both input dataframes are retained, so each row in the output will have all the columns from the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veg_remnants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, lets combine all polygons with the same threat status together using `dissolve` to simplify our geodataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all polygons with the saem threat status into one\n",
    "veg_remnants_simple = veg_remnants.dissolve('RLE2021')\n",
    "#view\n",
    "veg_remnants_simple.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is tons more functionality in GeoPandas, you can spatially join geodatframes with `.sjoin()`, reproject using `to_crs()`, and do all the good stuff you would expect. Two great places to dive deeper are the [GeoPandas user guide](https://geopandas.org/en/stable/docs/user_guide.html), and [the Carpentries lesson on vector data in Python](https://carpentries-incubator.github.io/geospatial-python/07-vector-data-in-python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gridded data with Xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### xarray concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Xarray](https://docs.xarray.dev/en/stable/) is the meat and potatoes of working with multidimensional gridded data in Python. While numpy provides many of the core operations we need for working with gridded data like indexing  matrix operations, etc it does not provide the functionality to add information about the various dimensions of arrays, the coordinates of grid cells, or attached important metadata. This is where Xarray comes in.\n",
    "\n",
    "By including labels on array dimensions Xarray opens up many new possibilities:\n",
    "\n",
    "- applying operations over dimensions by name: x.sum('time').\n",
    "\n",
    "- selecting values by label x.sel(time='2014-01-01').\n",
    "\n",
    "- use the split-apply-combine paradigm with groupby: x.groupby('time.dayofyear').mean().\n",
    "\n",
    "- keeping track of arbitrary metadata in the form of a Python dictionary: x.attrs.\n",
    "\n",
    "- and much more\n",
    "\n",
    "The Xarray data structure makes it trivial to go from 2 to 3 to 4 to N dimensions, hence it is a great choice for working with imaging spectroscopy data where we will have at least 3 (lat, lon, wavelength) dimensions. Another big benefit is that it seamlessly integrates with `Dask` a popular library for parallel computing in Python. This allows us to scale analysis with Xarray to very large data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core data structure of Xarray is an `xarray.DataArray` - which in its simplest form is just a Numpy array with named dimensions and coordinates on those dimensions. We can combine multiple `xarray.DataArray` in a single structure called a `xarray.Dataset`. Let's see what this looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#typically we use the xr aliais\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "#create a 2x3 np array\n",
    "arr = np.random.randn(2, 3)\n",
    "\n",
    "#create a xarray.DataArray by naming the dims and giving them coordinates\n",
    "xda = xr.DataArray(arr,\n",
    "                    dims=(\"x\", \"y\"),\n",
    "                    coords={\"x\": [10, 20],\n",
    "                            \"y\": [1.1,1.2,1.3]})\n",
    "\n",
    "xda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the individual components like the data itself, the dimension names or the coordinates using accessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the actual data\n",
    "print(xda.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get teh dimenson names\n",
    "print(xda.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the x coordinates\n",
    "print(xda.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set or get any metadata attribute we like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda.attrs[\"long_name\"] = \"random mesurement\"\n",
    "xda.attrs[\"random_attribute\"] = 123\n",
    "\n",
    "print(xda.attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and perform calculations on `xarray.DataArrays` as if they were Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(xda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An `xarray.Dataset` is a container of multiple aligned DataArray objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataarray with aligned dimensions (but it can be more or fewer dims)\n",
    "#create a new 2x3x4 xarray Dataarray\n",
    "arr2 = np.random.randn(2, 3, 4)\n",
    "xda2 = xr.DataArray(arr2,\n",
    "                    dims=(\"x\", \"y\",\"z\"),\n",
    "                    coords={\"x\": [10, 20],\n",
    "                            \"y\": [1.1,1.2,1.3],\n",
    "                            \"z\": [20,200,2000,20000]})\n",
    "\n",
    "#combine with another xarray.DataArray to make a xarray.Dataset\n",
    "xds = xr.Dataset({'foo':xda,'bar':xda2})\n",
    "xds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that we have multiple arrays in a single dataset. Xarray automatically aligns the arrays based on shared dimensions and coodrinates. You can do almost everything you can do with DataArray objects with Dataset objects (including indexing and arithmetic) if you prefer to work with multiple variables at once. You can also easily retrieve a single DataArray by name from a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xds.foo\n",
    "# xds['foo'] works the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Terminology\n",
    "It is important to be precise with our terminology when dealing with Xarrays as things can quickly get confusing when working with many dims. The full glossary can be found [here](https://docs.xarray.dev/en/stable/user-guide/terminology.html), but a quick recap:\n",
    "- `xarray.DataArray` - A multi-dimensional array with labeled or named dimensions\n",
    "- `xarray.Dataset` - A collection of DataArrays with aligned dimensions\n",
    "- **Dimension** - The (named) axes of an array\n",
    "- **Coordinate** - An array that labels a dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](xarray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading data from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xarray supports reading and writing of several file formats, from simple Pickle files to the more flexible netCDF format. The recommended way to store Xarray data structures is netCDF. Xarray is based on the netCDF data model, so netCDF files on disk directly correspond to Dataset objects. If you aren’t familiar with this data format, the [netCDF FAQ](https://www.unidata.ucar.edu/software/netcdf/docs/faq.html#What-Is-netCDF) is a good place to start. When we are working with complex multidimensional data, file formats start to matter a lot, and they make a big difference to how fast and efficiently we can load and analyse data. More on this in the next lesson.\n",
    "\n",
    "We can load netCDF files to create a new Dataset using `open_dataset()`. Similarly, a DataArray can be saved to disk using the `DataArray.to_netcdf()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this lesson we will work with a small dataset from a [Specim FENIX](https://www.specim.com/products/fenix/) airbone imaging spectrometer collected near the town of Franschoek, near Cape Town in 2018. Lots of important metadata about the image has been removed to keep this simple. All that remains are the measured reflectances, and the latitude, longitude and wavelength coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_is = xr.open_dataset(\"shared/users/gmoncrieff/data/is_example.nc\")\n",
    "xda_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### indexing, selecting and masking\n",
    "\n",
    "While you can use numpy-like indexing e.g `da[:,:]`, this does not make use of the power of having named dims and coords. Xarray as specific method for selecting using the position in the array `.isel()` and using the coordinates with `.sel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idexing using position\n",
    "xda_is.isel(x=20,y=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract a continous slice of an array dimension using `slice()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_is.isel(x=20,y=20,wl=slice(0,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use all the same techniques, but provive coordinate values rather than positions if we use `.sel()`. We can also provide an option for what to do if we do not get an exact match to the provided coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_is.sel(x=3.175e+05,y=6.263e+06,method='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mask values in our array using conditions based on the array values or coordinate values with `.where()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop bad bands\n",
    "xda_is = xda_is.where(xda_is.wl < 2.1,drop=True)\n",
    "xda_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that often it takes almost no time at all to run xarray code. This is because for many functions xarray does not load data from disk and actually perform the calculation, rather it simply prints a summary and high-level overview of the data that will be produced. This is called **Lazy computation** and is the smart thing to do when working with large datasets. Only when you really need to do the calculation does it actually happen - like when calling `.plot()` or writing results. We can force computation by running `xarray.DataArray.compute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_is.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Chunks\n",
    "When opening our data we can specific that we want the data split into chunks along each dimension like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_chunk = xr.open_dataset(\"data/is_example.nc\",chunks={'x':50,'y':50,'wl':-1})\n",
    "xda_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What does this do, and why should we do it?\n",
    "If you don't specify that you want the dataset chunked, xarray will load all the data into a numpy array. This can be okay if you are working witha small dataset but as your data grows larger chunking has a number of advantages:\n",
    " \n",
    " - __Efficient Memory Usage__\n",
    " Without chunking, xarray loads the entire dataset into memory as NumPy arrays, which can use a lot of RAM and may cause your system to slow down or crash. Chunking splits the data into smaller pieces, allowing you to work with datasets that are bigger than your available memory by loading only what you need.\n",
    " \n",
    " - __Better Performance__\n",
    "Processing smaller chunks can speed up computations and make data handling more efficient.Data is loaded into memory only when required, reducing unnecessary memory usage and improving processing speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the [dask documentation on chunks](https://docs.dask.org/en/latest/array-chunks.html) to find out more about chunking your array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default chunking and rechunking\n",
    "Some file types like netCDF or zarr have native chunking, and it is usually most efficient to use the chunking that is already present. If you specify `chunks='auto'` chunking will be automatically determined. This is a major advantage as chunking/rechunking can be expensive for large files. The downside is that you are subject to the chunking chosen by the creator of the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will remember that forcing computation using `compute()` returned a Numpy array. If we want to force computation but keep the resulting array as a chunked array we can use `persist()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the example from above\n",
    "xda_chunk = xda_chunk.where(xda_chunk.wl < 2.1,drop=True)\n",
    "#persist instead of compute\n",
    "xda_chunk.persist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make xarray geospatial with rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have latitude and longitude values associated with our Xarray, this data is not a proper geospatial dataset and hence we cannot do spatial manipulations like calculating distances or reprojecting. Xarray is a general-purpose tool for any multidimensional data and is not specific to geospatial data. We need an additional package `rioxarray` which brings all of the power of `GDAL` to Xarrays. `rioxarray` extends Xarray with the `rio` accessor. What this means is that a bunch of new functions become available to Xarray instances by typing `xarray.DataArray.rio.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and most important detail we need to add before turning our Xarray into a geospatial dataset is information about the projection. Here we know the current crs is epsg:32734 (UTM zone 34S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_chunk = xda_chunk.rio.write_crs('epsg:32734')\n",
    "xda_chunk.rio.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know the current projection, it is easy to reproject to match the projection of the vector data we were working with earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xda_chunk_wgs = xda_chunk.rio.reproject('epsg:4326')\n",
    "xda_chunk_wgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rioxarray gives us the ability to perform a numbr of spatial operations e.g. clip and mosaict, and read and write any file format supported by GDAL. This is as simple as `rioxarray.open_rasterio()` and `xarray.DataArray.rio.to_raster()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### credits:\n",
    "\n",
    "This lesson has borrowed heavily from:  \n",
    "\n",
    "[https://github.com/data-psl/lectures2020](https://github.com/data-psl/lectures2020)\n",
    "\n",
    "[The Carpentries Geospatial Python lesson by Ryan Avery](https://carpentries-incubator.github.io/geospatial-python/)  \n",
    "\n",
    "[The geopandas user guide](https://geopandas.org/en/stable/docs/user_guide.html)  \n",
    "\n",
    "[The xarray user guide](https://docs.xarray.dev/en/stable/user-guide/index.html) \n",
    "\n",
    "[An Introduction to Earth and Environmental Data Science](https://earth-env-data-science.github.io/intro.html)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
